{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Fashion MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# import mnist_reader\n",
    "def load_mnist(path, kind):\n",
    "    import os\n",
    "    import gzip\n",
    "    import numpy as np\n",
    "\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels-idx1-ubyte.gz'\n",
    "                               % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images-idx3-ubyte.gz'\n",
    "                               % kind)\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
    "                               offset=8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
    "                               offset=16).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "train_set, train_labels = load_mnist('/Users/macbookpro/Desktop/MLfinalproj/fashion-mnist-master/data/fashion', kind='train')\n",
    "test_set, test_labels = load_mnist('/Users/macbookpro/Desktop/MLfinalproj/fashion-mnist-master/data/fashion', kind='t10k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the dataset\n",
    "#### I took the first 10,000 samples from the 60,000 sample dataset, transposed the matrix so the columns would contain each sample and the rows are the features and I then centered and normalized the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_train[0:1000].shape)\n",
    "# np.random.shuffle(np.transpose(train_set))\n",
    "# print(train_set.shape)\n",
    "X_new_train = train_set[0:10000]\n",
    "new_labels = train_labels[0:10000]\n",
    "# X_new_train = X_new_train.transpose()\n",
    "# Arranging training data so that each sample is a column in the data matrix\n",
    "X = X_new_train.T # Data matrix\n",
    "# X = X_train[0:1000]\n",
    "# Computation of the empirical mean of data, both as a vector and as a tiled matrix\n",
    "mean_vec = X.mean(axis = 1)\n",
    "mean_mat = np.tile(mean_vec.reshape(X.shape[0],1),[1,X.shape[1]])\n",
    "\n",
    "# Centered data matrix\n",
    "centered_X = X - mean_mat\n",
    "\n",
    "# Arranging test data so that each sample is a column in the data matrix\n",
    "test_set = test_set.T\n",
    "\n",
    "# Appropriate shaping of the empirical mean as a tiled matrix whose dimensions match those of test data matrix\n",
    "mean_mat = np.tile(mean_vec.reshape(test_set.shape[0],1),[1,test_set.shape[1]])\n",
    "\n",
    "# Centered test data matrix\n",
    "centered_X_test = test_set - mean_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "#### I decided to reduce the dimensions of my dataset for k-NN in order to be able to run the algorithm on my own computer, this reduced the features down to 184 when taking the features that accounted for 95% variation of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Singular value decomposition of X\n",
    "U, s, Vh = np.linalg.svd(centered_X)\n",
    "# Computation of an appropriate 'r'\n",
    "r = 0; cum_sum = 0\n",
    "data_energy = np.linalg.norm(centered_X)**2\n",
    "for i in range(len(s)):\n",
    "    cum_sum = cum_sum + s[i]**2\n",
    "    if cum_sum/data_energy >= 0.95:\n",
    "        r = i+1\n",
    "        break\n",
    "        \n",
    "# Print the value of r\n",
    "# display(Latex(r'The calculated value of the integer $r$ is {}.'.format(r)))\n",
    "\n",
    "# Matrix of the top-r principal components of the centered data matrix\n",
    "U_r = U[:,0:r]\n",
    "\n",
    "# Compute the PCA-based features of the centered data using the top-r principal components\n",
    "X_tilde = U_r.T@centered_X\n",
    "\n",
    "# # Compute the PCA-based features of the centered test data using the top-r principal components\n",
    "X_tilde_test = U_r.T@centered_X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kNN(train, train_labels, k, test_x):\n",
    "#     train = np.delete(train, np.s_[testval:testval+1], 1)\n",
    "#     print(train.shape)\n",
    "    # Compute distances between all training samples and the test sample\n",
    "#     dist = np.array([np.linalg.norm(train[:,testval] - train[:,i]) for i in range(train.shape[1])])\n",
    "    \n",
    "    dist = np.array([np.linalg.norm(test_x - train[:,i]) for i in range(train.shape[1])])\n",
    "    \n",
    "    # Sort the distances and find the indices of the k-NNs\n",
    "    sort_dist = dist.argsort() # Sorting\n",
    "#     print(sort_dist)\n",
    "    kNN_labels = train_labels[sort_dist[0:k]] # Indices\n",
    "#     print(kNN_labels)\n",
    "    estlabels = np.zeros(10)\n",
    "    for i in range(len(kNN_labels)):\n",
    "        estlabels[kNN_labels[i]] += 1\n",
    "#     print(estlabels)\n",
    "    return np.argmax(estlabels)\n",
    "#     est_labels[0] = 1;\n",
    "    # Return the label that occurs most frequently within the k-NNs\n",
    "#     if np.size(np.where(kNN_labels==0)) >= np.size(np.where(kNN_labels==1)):\n",
    "#         return 0\n",
    "#     else:\n",
    "#         return 1\n",
    "#     print(kNN_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kNN(X_tilde, new_labels, 141, 3)\n",
    "est_labels = np.array([kNN(X_tilde,new_labels,100,X_tilde_test[:,i]) for i in range(1000)])\n",
    "\n",
    "# print(est_labels)\n",
    "# print(test_labels[0:1000])\n",
    "acc_label = np.equal(est_labels,test_labels[0:1000])\n",
    "# print(acc_label)\n",
    "# Calculation of the average classification error\n",
    "ave_clf_err = (np.size(np.where(acc_label==False)))/acc_label.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The average classification error of k-NN on this dataset came out to be about 0.19, if I could run the entire dataset on my computer I believe the error would be a lot smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.195\n"
     ]
    }
   ],
   "source": [
    "print(ave_clf_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "### For logistic regression I chose to pick the first two classes of my dataset, class 0 and class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 784)\n",
      "(12000,)\n",
      "(167, 10000)\n",
      "(167,)\n"
     ]
    }
   ],
   "source": [
    "X_new = []\n",
    "labels = []\n",
    "for i in range(train_set.shape[0]):\n",
    "    if(train_labels[i] == 0):\n",
    "        X_new.append(train_set[i])\n",
    "        labels.append(0)\n",
    "    elif(train_labels[i] == 1):\n",
    "        X_new.append(train_set[i])\n",
    "        labels.append(0)\n",
    "X_new = np.array(X_new)\n",
    "labels = np.array(labels)\n",
    "print(X_new.shape)\n",
    "print(labels.shape)\n",
    "X_new = X_new.T\n",
    "\n",
    "X_test = []\n",
    "testlabels = []\n",
    "for i in range(test_set.shape[0]):\n",
    "    if(test_labels[i] == 0):\n",
    "        X_test.append(test_set[i])\n",
    "        testlabels.append(0)\n",
    "    elif(test_labels[i] == 1):\n",
    "        X_test.append(test_set[i])\n",
    "        testlabels.append(0)\n",
    "        \n",
    "X_test = np.array(X_test)\n",
    "testlabels = np.array(testlabels)\n",
    "print(X_test.shape)\n",
    "print(testlabels.shape)\n",
    "X_test = X_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \n",
    "    s = 1.0 / (1.0 + np.exp(-z))\n",
    "    \n",
    "    return s\n",
    "\n",
    "def initialize(dim):\n",
    "    w = np.zeros((dim,1))\n",
    "    b = 0\n",
    "    \n",
    "    assert (w.shape == (dim,1))\n",
    "    assert (isinstance(b, float) or isinstance(b,int))\n",
    "    \n",
    "    return w,b\n",
    "\n",
    "\n",
    "def propagate(w, b, X, Y):\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    z = np.dot(w.T,X)+b\n",
    "    A = sigmoid(z)\n",
    "    cost = -1.0/m*np.sum(Y*np.log(A)+(1.0-Y)*np.log(1.0-A))\n",
    "    \n",
    "    dw = 1.0/m*np.dot(X, (A-Y).T)\n",
    "    db = 1.0/m*np.sum(A-Y)\n",
    "    \n",
    "    assert (dw.shape == w.shape)\n",
    "    assert (db.dtype == float)\n",
    "    \n",
    "    cost = np.squeeze(cost)\n",
    "    assert (cost.shape == ())\n",
    "\n",
    "    return grads, cost\n",
    "\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        w = w - learning_rate*dw\n",
    "        b = b - learning_rate*db\n",
    "        \n",
    "    return params, grads\n",
    "\n",
    "def predict (w, b, X):\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0],1)\n",
    "    \n",
    "    A = sigmoid (np.dot(w.T, X)+b)\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        if (A[:,i] > 0.5): \n",
    "            Y_prediction[:, i] = 1\n",
    "        elif (A[:,i] <= 0.5):\n",
    "            Y_prediction[:, i] = 0\n",
    "            \n",
    "    assert (Y_prediction.shape == (1,m))\n",
    "    \n",
    "    return Y_prediction\n",
    "\n",
    "def model (X_train, Y_train, X_test, Y_test, num_iterations = 1000, learning_rate = 0.5, print_cost = False):\n",
    "    \n",
    "    w, b = initialize(X_train.shape[0])\n",
    "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "    \n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    Y_prediction_test = predict (w, b, X_test)\n",
    "    Y_prediction_train = predict (w, b, X_train)\n",
    "    \n",
    "    train_accuracy = 100.0 - np.mean(np.abs(Y_prediction_train-Y_train)*100.0)\n",
    "    test_accuracy = 100.0 - np.mean(np.abs(Y_prediction_test-Y_test)*100.0)\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = model (X_new, labels, X_test, testlabels[0], num_iterations = 4000, learning_rate = 0.05, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(182, 10000)\n",
      "(10000, 182)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (784,) (182,) (784,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-180-61d01dca37da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mpopulation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mmean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mX_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mmean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (784,) (182,) (784,) "
     ]
    }
   ],
   "source": [
    "cluster = np.random.randint(10,size=10000)\n",
    "# mean = np.zeros((3,2))\n",
    "# population = np.zeros((3,1))\n",
    "\n",
    "print(X_tilde.shape)\n",
    "X_t = X_tilde.T\n",
    "print(X_t.shape)\n",
    "change = True\n",
    "num_of_iter = 0\n",
    "while(change):\n",
    "    population = np.zeros((10,1))\n",
    "    mean = np.zeros((10,784))\n",
    "# print(mean.shape)\n",
    "    for i in range(10000):\n",
    "        population[cluster[i]] += 1\n",
    "        mean[cluster[i]] += X_t[i]\n",
    "    for i in range(10):\n",
    "        mean[i] = mean[i]/population[i]\n",
    "    change = False\n",
    "    for i in range(10000):\n",
    "        for j in range(10):\n",
    "            if j != cluster[i]:\n",
    "                if np.linalg.norm(X_t[i]-mean[j]) < np.linalg.norm(X_t[i]-mean[cluster[i]]):\n",
    "                    cluster[i] = j\n",
    "                    change = True\n",
    "    num_of_iter += 1\n",
    "print(population)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-means converged after  0  iterations\n",
      "(10000,)\n",
      "0.8987\n"
     ]
    }
   ],
   "source": [
    "print(\"k-means converged after \", num_of_iter, \" iterations\")\n",
    "print(cluster.shape)\n",
    "acc_label = np.equal(cluster,new_labels)\n",
    "ave_clf_err = (np.size(np.where(acc_label==False)))/acc_label.size\n",
    "print(ave_clf_err)\n",
    "# x0 = []\n",
    "# x1 = []\n",
    "# x2 = []\n",
    "# for i in range(3000):\n",
    "#     if cluster[i] == 0:\n",
    "#         x0.append(X[i])\n",
    "#     elif(cluster[i] == 1):\n",
    "#         x1.append(X[i])\n",
    "#     elif(cluster[i] == 1):\n",
    "#         x1.append(X[i])\n",
    "#     elif(cluster[i] == 1):\n",
    "#         x1.append(X[i])\n",
    "#     else:\n",
    "#         x2.append(X[i])\n",
    "        \n",
    "# x0 = np.asarray(x0)\n",
    "# x1 = np.asarray(x1)\n",
    "# x2 = np.asarray(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (182,1) (10000,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-182-ebebd9a17541>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# one epoch is one iteration through the complete dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mX_tilde\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mprod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (182,1) (10000,1) "
     ]
    }
   ],
   "source": [
    "w = np.zeros(X_tilde.shape)\n",
    "epochs = 1\n",
    "alpha = 0.0001 # learning rate\n",
    "\n",
    "while(epochs < 10000): # one epoch is one iteration through the complete dataset\n",
    "    y = np.sum(w * X_tilde, axis = 1)\n",
    "    prod = y.reshape((-1,1)) * new_labels.reshape((-1,1))\n",
    "    if(epochs % 1000 == 0):\n",
    "        print(epochs)\n",
    "    index = 0\n",
    "    for val in prod:\n",
    "        if(val >= 1):\n",
    "            cost = 0\n",
    "            w = w - alpha * (2 * 1/epochs * w) # updating the parameters\n",
    "            \n",
    "        else:\n",
    "            cost = 1 - val\n",
    "            w = w + alpha * (X_tilde[index] * new_labels[index] - 2 * 1/epochs * w) # iterating over all points\n",
    "        index += 1\n",
    "    epochs += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
